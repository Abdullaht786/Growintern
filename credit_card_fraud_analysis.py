# -*- coding: utf-8 -*-
"""Credit Card Fraud Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/credit-card-fraud-analysis-3d21ab3b-e5c1-44cc-9ac1-8a43952b16bc.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241015/auto/storage/goog4_request%26X-Goog-Date%3D20241015T070819Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4b282c9c087c33179ed38888de5c522cc1fe8804588638b6a13650bb6d5a53ff625e47558219422c147611b8df3504e03a8869816f2acb240e6077aae06e548ba62a44ca410e946a568c59c047becaa9b5b1feaeb9ed0dce58d7f7afdbd1d3e052e0d2c0add0dc8595848321acfc697187854468e4f005ed4a494b6960fcb2fcce9e57a1ca9220ba0e1ebc2cef7cd7b7667dd96b07b54b7240e3383b77227d501af2d74fd58900307d3c41177b732a1e4666321426ba55bd684eb9228310bd46e748b6eb8342c9d8b3d40d6b181a21b0e718396d1a895d1a9e01b324517ca73fccfb526dc30e2ef30796498f89c58b08cd6a8995eb9c5b406c6ea47e0cd9d9d4
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = '/kaggle/input/creditcardfraud/creditcard.csv'  # Replace with the correct file path
df = pd.read_csv(file_path)

# Step 1: Check the first few rows of the dataset
print("First few rows of the dataset:")
print(df.head())

# Step 2: Check for missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

# Step 3: Analyze the class distribution (fraud vs non-fraud)
print("\nClass distribution:")
print(df['Class'].value_counts())
class_counts = df['Class'].value_counts()
plt.figure(figsize=(6, 4))
sns.barplot(x=class_counts.index, y=class_counts.values, palette='viridis')
plt.title("Class Distribution (Fraud vs Non-Fraud)")
plt.xticks([0, 1], ['Non-Fraud (0)', 'Fraud (1)'])
plt.ylabel('Count')
plt.show()

# Step 4: Get descriptive statistics of the dataset
print("\nDescriptive statistics of the dataset:")
print(df.describe())

# Optional: Plot correlation heatmap to understand feature relationships
plt.figure(figsize=(12, 10))
corr = df.corr()
sns.heatmap(corr, cmap='coolwarm', annot=False, fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

"""**Explanation:**

1. **Loading the dataset:** The script loads the dataset into a Pandas DataFrame.
2. **Checking for missing values:** This step identifies if any columns contain missing values.
3. **Class distribution:** This part analyzes the distribution of fraudulent (Class = 1) vs non-fraudulent (Class = 0) transactions and visualizes it using a bar plot.
4. **Descriptive statistics:** It calculates summary statistics (mean, std, min, max, etc.) for each feature.
5. **Correlation heatmap (optional):** It provides a visual overview of feature correlations to help understand the relationships between them.

This initial exploration will give us a good sense of the data. You can run this on the sample or full dataset. Let me know how you would like to proceed next!
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Step 3: Analyze the class distribution (fraud vs non-fraud)
class_counts = df['Class'].value_counts()

# Print the class distribution
print("\nClass distribution (0: Non-Fraud, 1: Fraud):")
print(class_counts)

# Plot the class distribution
plt.figure(figsize=(6, 4))
sns.barplot(x=class_counts.index, y=class_counts.values, palette='viridis')
plt.title("Class Distribution (Fraud vs Non-Fraud)")
plt.xticks([0, 1], ['Non-Fraud (0)', 'Fraud (1)'])
plt.ylabel('Count')
plt.xlabel('Class')
plt.show()

"""For cross-validation and model evaluation, we can use a few popular machine learning algorithms and evaluate them based on metrics that are useful for imbalanced datasets. Specifically, we will use Precision, Recall, F1-Score, and the Area Under the Precision-Recall Curve (AUPRC), which is more informative for imbalanced data like fraud detection.

Here’s how we can proceed with cross-validation and model evaluation:

Steps:
1. **Prepare the data:** Split the data into features (`X`) and labels (`y`).
2. **Cross-validation setup:** Use `StratifiedKFold` to ensure each fold of cross-validation maintains the class imbalance.
3. **Train models:** Use Logistic Regression and RandomForestClassifier as example models.
4. **Evaluate models:** Calculate Precision, Recall, F1-Score, and AUPRC.

"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import classification_report, precision_recall_curve, auc
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score

# Step 1: Prepare the data (separate features X and labels y)
X = df.drop(columns=['Class'])
y = df['Class']

# Step 2: Cross-validation setup
# Use StratifiedKFold to maintain class distribution in each fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Models to test
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
#     "Random Forest": RandomForestClassifier(n_estimators=100)
}

# Step 3: Train and evaluate each model
for model_name, model in models.items():
    print(f"\nEvaluating {model_name}...")
    precision_scores = []
    recall_scores = []
    f1_scores = []
    auprc_scores = []

    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # Train the model
        model.fit(X_train, y_train)

        # Predict on test data
        y_pred = model.predict(X_test)

        # Precision, Recall, F1 Score
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        # Precision-Recall AUC
        y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for positive class
        precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_prob)
        auprc = auc(recall_curve, precision_curve)

        precision_scores.append(precision)
        recall_scores.append(recall)
        f1_scores.append(f1)
        auprc_scores.append(auprc)

    # Step 4: Print evaluation results
    print(f"\nModel: {model_name}")
    print(f"Average Precision: {np.mean(precision_scores):.4f}")
    print(f"Average Recall: {np.mean(recall_scores):.4f}")
    print(f"Average F1-Score: {np.mean(f1_scores):.4f}")
    print(f"Average AUPRC: {np.mean(auprc_scores):.4f}")

"""**Explanation:**
1. **StratifiedKFold:** We use `StratifiedKFold` to split the data into 5 folds, ensuring that each fold maintains the same class imbalance as the original dataset.
2. **Model training and evaluation:** Two models (Logistic Regression and Random Forest) are trained and evaluated across the 5 cross-validation folds. For each fold, we calculate Precision, Recall, F1-Score, and AUPRC.
3. **Model evaluation metrics:**
* **Precision:** The ratio of correctly predicted frauds to the total predicted frauds.
* **Recall:** The ratio of correctly predicted frauds to the total actual frauds.
* **F1-Score:** The harmonic mean of Precision and Recall.
* **AUPRC:** The Area Under the Precision-Recall Curve, which is particularly useful for imbalanced datasets.

Feature engineering is a crucial part of improving model performance. In this step, we will create new features and refine existing ones to help the model better detect fraud. Given that the dataset contains mainly numerical features (including PCA-transformed components), we can focus on engineering time-based features and possibly interaction features.

Here are some feature engineering ideas:

1. Time-Based Features:
* Transaction Frequency: Measure how many transactions happen within specific time windows (e.g., the number of transactions in the last hour).
* Time since Last Transaction: Calculate the time difference between consecutive transactions.
* Day/Night Transactions: Determine whether transactions occur during business hours or outside of them, which could be an indicator of fraud.
2. Amount-Based Features:
* Transaction Amount Ranges: Create a feature to bucket transactions based on the amount (e.g., low, medium, high).
* Relative Transaction Amount: Compare each transaction amount to the median amount of transactions for the same user or within a certain time window.
3. Feature Interaction:
* Interaction Between PCA Components: We can combine multiple PCA features to create interaction terms that may capture more complex patterns.
"""

import pandas as pd
import numpy as np

# Step 4: Feature Engineering

# 1. Time-Based Features
# Convert 'Time' to hours
df['Hour'] = df['Time'] / 3600  # Convert seconds to hours

# Time since last transaction
df['Time_Since_Last_Transaction'] = df['Time'].diff().fillna(0)

# 2. Amount-Based Features
# Log-transform the 'Amount' feature to reduce skewness
df['Log_Amount'] = np.log1p(df['Amount'])

# Bucket transaction amounts into categories
df['Amount_Category'] = pd.cut(df['Amount'], bins=[0, 1, 10, 100, 1000, np.inf], labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])

# 3. Feature Interaction
# Interaction between V1 and V2 as an example
df['V1_V2_Interaction'] = df['V1'] * df['V2']

# Optional: You can create more interactions between different features
# For example, between different PCA components
df['V1_V3_Interaction'] = df['V1'] * df['V3']
df['V2_V4_Interaction'] = df['V2'] * df['V4']

# Step 4: Check the new features
print("\nFirst few rows with new features:")
print(df[['Time', 'Hour', 'Time_Since_Last_Transaction', 'Amount', 'Log_Amount', 'Amount_Category', 'V1_V2_Interaction']].head())

"""# Set Thresholds for Fraud Detection

* After training the model and evaluating its performance, one key step is to determine the right threshold for classifying a transaction as fraudulent.
* Why? Most fraud detection models will output a probability that a transaction is fraudulent, and you’ll need to define a threshold where transactions are flagged for further review (e.g., transactions with a probability higher than 0.8 are flagged).
* How? This can be adjusted based on the business’s tolerance for fraud vs. the cost of investigating false positives.

Here’s an example code snippet to determine the optimal threshold:
"""

from sklearn.metrics import precision_recall_curve

# Assume you already have the predicted probabilities from the model
y_prob = model.predict_proba(X_test)[:, 1]  # Predicted probabilities of class 1 (fraud)

# Get precision-recall curve values
precision, recall, thresholds = precision_recall_curve(y_test, y_prob)

# Calculate F1 score for each threshold to find the optimal point
f1_scores = 2 * (precision * recall) / (precision + recall)
optimal_threshold = thresholds[np.argmax(f1_scores)]

print(f"Optimal Threshold for Fraud Detection: {optimal_threshold}")

"""# User Behavior Profiling

* Behavioral Analytics: Monitor the usual behavior of users (e.g., transaction amount, location, time of day) and detect anomalies that indicate unusual activity.
* How? For instance, transactions happening at odd hours or in locations where the user has never shopped could trigger alerts. These behavioral profiles can be dynamically updated and compared against real-time transactions.
"""

# Example: Calculate z-score for a new transaction's amount based on historical data
from scipy.stats import zscore

# Assuming df is a DataFrame containing previous transactions for a specific user
df['Amount_Z_Score'] = zscore(df['Amount'])

# A new transaction with amount
new_transaction_amount = 500  # Example

# Calculate z-score for the new transaction
mean_amount = df['Amount'].mean()
std_amount = df['Amount'].std()
z_score_new = (new_transaction_amount - mean_amount) / std_amount

if z_score_new > 3:  # Threshold for unusual behavior (3 standard deviations from the mean)
    print("Transaction flagged as unusual based on amount.")